{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'Hands-on Session 1: Uplift Modeling'\n",
        "format: html\n",
        "execute: \n",
        "  eval: false\n",
        "  echo: true\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "In this notebook, we will apply Double Machine Learning in a use case of uplift modeling. Due to the general goal of the course will consider a generated dataset. The data generating process is calibrated based on empirical examples and common simulations in research. \n",
        "\n",
        "The goal is to mimic an observational dataset, which is derived from from a marketing campaign with the goal of increasing the conversion rate of a product.\n",
        "\n",
        "## Scenario\n",
        "\n",
        "We consider a scenario with an online shop that wants to evaluate the effect of their email campaigns. They send regular emails to their newsletter subscribers, in which they offer a discount for a specific product. The goal is to increase the conversion rate of the product. So far, they always sent the same email to all subscribers, i.e., every subscriber receives a coupon. Now, they want to investigate if a new strategy could be more effective, i.e., by targeting the coupons towards specific subgroups of their subscribers. The rationale behind this is that some subcribers would have bought the product anyway, even without the discount. Hence, they evaluate their historical (non-experimental) sales data over the last months.\n",
        "\n",
        "\n",
        "Lets start by importing the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "import doubleml as dml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data\n",
        "\n",
        "Load our dataset and examine all available features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "url = 'https://docs.doubleml.org/tutorial/stable/datasets/data/uplift_data.csv'\n",
        "df = pd.read_csv(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset contains $10,000$ observations and $22$ variables. The variables are\n",
        "\n",
        " - `conversion`: the outcome of interest (binary)\n",
        " - `coupon`: the treatment variable, whether the customer used a coupon (binary)\n",
        " - `X1`-`X14`: A set of continuous features (numeric), measuring information on age, previous activities, time of last purchase, etc.\n",
        " - `membership_level`: Dummy coded categorical feature for information on membership level (binary)\n",
        " - `Z`: A score measuring customers' activity 2 months after the campaign (numeric)\n",
        " - `ite`: The individual treatment effect (numeric)\n",
        "\n",
        "\n",
        "All continuous variables have been normalized before our analysis.\n",
        "\n",
        "Of course, in a real-world application, we would not have the individual effect `ite` available. We will use it here to evaluate the performance of our model.\n",
        "\n",
        "The goal is to estimate the effect of the treatment variable `coupon` on the the `conversion` probability. Note that in this setting, both treatment and outcome are binary variables.\n",
        "\n",
        "As a first impression, let's evaluate the difference in averages between the treatment and \"control\" group and compare it to the average treatment effect in the sample. You can use the `ttest_ind` from `scipy.stats` to test for the statistical significance of the difference in means."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# True average treatment effect\n",
        "ATE = df['ite'].mean()\n",
        "print(ATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## TODO: Run significance test (as if data came from an experiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part I: Basic DoubleML\n",
        "\n",
        "We will follow the basic steps of the [DoubleML Workflow](https://docs.doubleml.org/stable/workflow/workflow.html).\n",
        "\n",
        "### Step 0: Problem Formulation & DAG\n",
        "\n",
        "As already mentioned, we are interested in estimating the effect of the treatment variable `coupon` on the the `conversion` probability. As the dataset is observational, we will have to decide which variables are confounders and what we have to control for.\n",
        "\n",
        "Usually it is very helpful to visualize the problem in a causal graph or directed acyclic graph (DAG). The following graph shows the causal graph for our problem.\n",
        "\n",
        "![DAG](figures/DAG_001.png)\n",
        "\n",
        "### Step 1: Data-Backend\n",
        "\n",
        "For the DoubleML-package, we have to prepare a specific [DoubleMLData object](https://docs.doubleml.org/stable/api/generated/doubleml.DoubleMLData.html).\n",
        "Please specify outcome variable, treatment variable, and covariates. The covariates should be a list of all variables that are used as confounders in the causal graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## TODO: Prepare DoubleMLData backend by specifying confounders, treatment variable and outcome"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Causal Model\n",
        "\n",
        "Next, we have to decide which [DoubleML Model](https://docs.doubleml.org/stable/guide/models.html) is appropriate for our problem. Which of the models would you choose and why?\n",
        "\n",
        "![](figures/doubleml_models_with_linear_score_classes_methods_py.png)\n",
        "\n",
        "As we have a single binary treatment variable and a single binary outcome variable (without instrumental variables), we will use the [DoubleMLIRM](https://docs.doubleml.org/dev/api/generated/doubleml.DoubleMLIRM.html) model.\n",
        "\n",
        "### Step 3: ML Methods\n",
        "\n",
        "As the use of machine learning methods is at the core of the DoubleML approach, we have to choose different learners to fit the nuisance parts of our score function. \n",
        "\n",
        "As a first step, take a look at the choosen model and the corresponding score function:\n",
        "\n",
        " - [DoubleMLIRM Model](https://docs.doubleml.org/stable/guide/models.html#interactive-regression-model-irm)\n",
        " - [DoubleMLIRM Score function](https://docs.doubleml.org/stable/guide/scores.html#interactive-regression-model-irm)\n",
        "\n",
        "The score shows that we have to estimate two nuisance components\n",
        "\n",
        "\\begin{align*}\n",
        "g_0(X) &= \\mathbb{E}[Y | X]\\\\\n",
        "m_0(X) &= \\mathbb{E}[D | X].\n",
        "\\end{align*}\n",
        "\n",
        "As both `conversion` and `coupon` are binary variables, the conditional expectations are conditional probabilities. Consequently it is only natural to use classification methods to estimate the nuisance components.\n",
        "\n",
        "As a basemodel for comparison, we will use a [logistic regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "ml_g_linear = LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000)\n",
        "ml_m_linear = LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define further learners for the nuisance components. Usual good choices are Random Forests (e.g. [scikit learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)) or Gradient Boosting Trees (e.g. [lightgbm](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html)). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## TODO: Specify learners of your choice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4 & 5: Initialize and estimate DoubleML Object\n",
        "\n",
        "Now we can initialize the [DoubleMLIRM](https://docs.doubleml.org/stable/api/generated/doubleml.DoubleMLIRM.html#doubleml.DoubleMLIRM) object.\n",
        "\n",
        "Add some hyperparameters such as\n",
        "\n",
        " - `n_folds`: number of folds for the cross-fitting\n",
        " - `n_rep`: number of repetitions for the cross-fitting\n",
        " - `trimming_threshold`: trimming threshold for the propensity score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## TODO: Specify a DoubleML model object"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Start the estimation procedure and take a look at the final estimates of your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## TODO: fit model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use additional learners for the nuisance components and compare the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## TODO: Specify and fit models that you specified yourself"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## TODO: Summarize the estimation output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can evaluate the performance of the nuisance learners by looking e.g. at the logloss (see Part 7.1.4 [documentation](https://docs.doubleml.org/stable/guide/learners.html#evaluate-learners))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Evaluate learners using log_loss or balanced_accuracy_score (or others)\n",
        "\n",
        "from sklearn.metrics import log_loss, balanced_accuracy_score\n",
        "def logloss(y_true, y_pred):\n",
        "    subset = np.logical_not(np.isnan(y_true))\n",
        "    return log_loss(y_true[subset], y_pred[subset])\n",
        "\n",
        "# print(f'Linear nuisance functions:\\n{dml_obj_linear.evaluate_learners(metric=logloss)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Inference\n",
        "\n",
        "Update the confidence interval to a $90\\%$ confidence interval."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## TODO: Estimate 90% confidence interval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding additional control variables\n",
        "\n",
        "Before, we controlled for all confounding variables. Next, repeat the analysis, but add the variables $X1$ - $X4$. \n",
        "\n",
        "Take a look at your DAG. What would you expect of the performance of the model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## TODO: Add additional controls and re-run analysis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## TODO: Summarize your results so far"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, add the variable $Z$. What is your expectation? Can you explain the results?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Initialize data-backend and causal model that includes Z as a confounder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Fit your model and summarize again"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Overall summary of estimates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## TODO: Summarize your findings so far"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Effect Heterogeneity\n",
        "\n",
        "The idea of uplift modeling is based on treatment effect heterogeneity. Let us take a look at the heterogeneity of the treatment effect in our sample.\n",
        "\n",
        "\n",
        "### Average Treatment Effect on the Treated\n",
        "\n",
        "As a first step to estimate heterogenous treatment effects, we will estimate the average treatment effect on the treated (ATTE)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## TODO: Estimate the ATTE by specifying \"score='ATTE'\" in the IRM model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The true effect among the treated is the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Compare your results to the true ATTE in the sample\n",
        "ATTE = df[df['coupon'] == 1]['ite'].mean()\n",
        "print(f'ATTE:\\n{ATTE}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Group Average Treatment Effects\n",
        "\n",
        "Let us only consider the boosting model with the additional control variables.\n",
        "\n",
        "Next, consider treatment effect heterogeneity. We will start with group treatment effects.\n",
        "Consider the the effects for the different membership level groups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Comparison to true GATEs in the sample\n",
        "for name in X_names_cat:\n",
        "    print(f\"{name}: {df['ite'][df[name] == 1].mean()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Try to use the `gate()` method to estimate the group specific treatment effects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Based on one of your previous models (with \"score = 'ATE'\"), estimate the GATE for membership levels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conditional Average Treatment Effects\n",
        "\n",
        "To consider the heterogeneity in continuous variables, we can specify groups based on bins or consider projections on a dictionary of basis functions.\n",
        "To start we will explain the basic idea using the individual treatment effect `ite`.\n",
        "\n",
        "The conditional average treatment effect (CATE), here conditionally on feature $X_5$ (e.g. age), is defined as \n",
        "\n",
        "$$\n",
        "\\tau(x) = \\mathbb{E}[\\underbrace{Y(1) - Y(0)}_{\\text{individual effect}} | X_5 = x].\n",
        "$$\n",
        "\n",
        "Assuming, we know the individual treatment effect, a natural estimator for the CATE is a linear projection on some basis functions $\\phi(X_5)$. In the simplest case, we just consider the linear projection on $X_5$ itself and an intercept.\n",
        "\n",
        "Why should we include the intercept?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "cate_var = 'X14'\n",
        "phi_x = sm.add_constant(df[cate_var])\n",
        "true_cate_linear = sm.OLS(df['ite'], phi_x).fit()\n",
        "print(true_cate_linear.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get a better grip on the idea, we will plot the CATE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.scatter(df[cate_var], df['ite'], alpha=0.3)\n",
        "plt.scatter(df[cate_var], true_cate_linear.predict(phi_x), color='green')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The feature $X_5$ does not seem to have a heterogenous treatment effect. Try to estimate the CATE with the DoubleML package.\n",
        "\n",
        "To predict the effect and the confidence interval you can use the `confint()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Estimate the CATE based on one of your DoubleML models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cate_confint_linear = cate_linear.confint(phi_x)\n",
        "print(cate_confint_linear)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.scatter(df[cate_var], true_cate_linear.predict(phi_x), color='green', s=0.5)\n",
        "plt.scatter(df[cate_var], cate_confint_linear['effect'], color='red', s=0.5)\n",
        "plt.scatter(df[cate_var], cate_confint_linear[ '2.5 %'], color='#FFC0CB', s=0.5)\n",
        "plt.scatter(df[cate_var], cate_confint_linear[ '97.5 %'], color='#FFC0CB', s=0.5)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To make the projection more complex, we can construct e.g. polynomial features of $X_5$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Create the polynomial features object\n",
        "poly = PolynomialFeatures(degree=3)\n",
        "phi_x_values = poly.fit_transform(df[[cate_var]])\n",
        "phi_x_polynomial = pd.DataFrame(phi_x_values, columns=poly.get_feature_names_out())\n",
        "\n",
        "true_cate_polynomial = sm.OLS(df['ite'], phi_x_polynomial).fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let us compare these values to the CATE results that we can obtain from our DoubleML object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## TODO: Estimate the CATE and its confidence intervals for your DoubleML object"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Visualize your CATE results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Policy Learning with Trees\n",
        "\n",
        "Let us now try to improve the coupon assignment mechanism by using a policy tree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# names of features for optimal policy\n",
        "policy_vars = ['X' + str(i) for i in range(10, 15)]  # X_names_cat \n",
        "\n",
        "# features for optimal policy\n",
        "policy_features = df[policy_vars].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Fit a shallow policy tree "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Plot the policy tree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can compare the suggested treatment assignment to the initial allocation of the coupons. We will load and use a separate data set for the evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "url_2 = 'https://docs.doubleml.org/tutorial/stable/datasets/data/uplift_data_policy.csv'\n",
        "df_policy = pd.read_csv(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Use the predict method to obtain an optimal policy assignment rule"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Estimate the ATTE for the policy data set as baseline comparison. \n",
        "\n",
        "# Hint: You can estimate the ATTE without re-estimation by just\n",
        "# using the treatment variable as the variable for the GATE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Can we improve upon the observed treatment assignment mechanism (in terms of the ATTE?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Compare the ATTE and the optimal policy ATTE. "
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}